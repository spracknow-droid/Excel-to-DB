import streamlit as st
import pandas as pd
import sqlite3
import os
import tempfile
import re  # íŠ¹ìˆ˜ë¬¸ì ì œê±°ìš©

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Excel & DB Merger", layout="wide")

st.title("ğŸ“Š í†µí•© ë°ì´í„° ë³€í™˜ê¸° (ì „í‘œë²ˆí˜¸ ê¸°ì¤€ ë¶„ë¥˜)")
st.markdown("'ìˆ˜ìµì„±ê³„íšì „í‘œë²ˆí˜¸' ê¸°ì¤€ ë¶„ë¥˜ ë° SQL í˜¸í™˜ì„± ê°•í™” ë²„ì „")

# --- ì‚¬ì´ë“œë°” ë° í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼) ---
st.sidebar.header("ğŸ“ ë°ì´í„° ì†ŒìŠ¤ ì—…ë¡œë“œ")
uploaded_plans = st.sidebar.file_uploader("1ï¸âƒ£ íŒë§¤ê³„íš (xlsx)", type=["xlsx"], accept_multiple_files=True, key="plan_uploader")
uploaded_results = st.sidebar.file_uploader("2ï¸âƒ£ íŒë§¤ì‹¤ì  (xlsx)", type=["xlsx"], accept_multiple_files=True, key="result_uploader")
uploaded_dbs = st.sidebar.file_uploader("3ï¸âƒ£ ê¸°ì¡´ SQLite (db)", type=["db"], accept_multiple_files=True, key="db_uploader")

all_data = []

def process_classification(df):
    if df is None or df.empty:
        return
    
    # ì»¬ëŸ¼ëª… ì•ë’¤ ê³µë°± ì œê±° (ë§¤ìš° ì¤‘ìš”)
    df.columns = [str(c).strip() for c in df.columns]
    
    if 'ìˆ˜ìµì„±ê³„íšì „í‘œë²ˆí˜¸' in df.columns:
        is_plan = df['ìˆ˜ìµì„±ê³„íšì „í‘œë²ˆí˜¸'].notnull() & (df['ìˆ˜ìµì„±ê³„íšì „í‘œë²ˆí˜¸'].astype(str).str.strip() != "")
        
        df_plan = df[is_plan].copy()
        if not df_plan.empty:
            df_plan = df_plan.rename(columns={'í’ˆëª…': 'í’ˆëª©ëª…', 'íŒë§¤ê¸ˆì•¡': 'ì¥ë¶€ê¸ˆì•¡'})
            qty = pd.to_numeric(df_plan.get('íŒë§¤ìˆ˜ëŸ‰', 0), errors='coerce').fillna(0)
            price = pd.to_numeric(df_plan.get('íŒë§¤ë‹¨ê°€', 0), errors='coerce').fillna(0)
            df_plan['íŒë§¤ê¸ˆì•¡'] = qty * price
            df_plan['__ë°ì´í„°êµ¬ë¶„__'] = "íŒë§¤ê³„íš"
            all_data.append(df_plan)
            
        df_result = df[~is_plan].copy()
        if not df_result.empty:
            df_result['__ë°ì´í„°êµ¬ë¶„__'] = "íŒë§¤ì‹¤ì "
            all_data.append(df_result)
    else:
        df_copy = df.copy()
        df_copy['__ë°ì´í„°êµ¬ë¶„__'] = "íŒë§¤ì‹¤ì "
        all_data.append(df_copy)

# --- íŒŒì¼ ë¡œë“œ êµ¬ê°„ (ìƒëµ, ê¸°ì¡´ê³¼ ë™ì¼) ---
if uploaded_plans or uploaded_results or uploaded_dbs:
    with st.status("ë°ì´í„° í†µí•© ì²˜ë¦¬ ì¤‘...", expanded=True) as status:
        # [ê¸°ì¡´ê³¼ ë™ì¼í•œ íŒŒì¼ ë¡œë“œ ë£¨í”„ ì‹¤í–‰...]
        for file in uploaded_plans:
            try: process_classification(pd.read_excel(file))
            except Exception as e: st.error(f"Plan Error: {e}")
        for file in uploaded_results:
            try: process_classification(pd.read_excel(file))
            except Exception as e: st.error(f"Result Error: {e}")
        for file in uploaded_dbs:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".db") as tmp_file:
                tmp_file.write(file.getvalue())
                tmp_path = tmp_file.name
            try:
                conn_old = sqlite3.connect(tmp_path)
                tables = pd.read_sql("SELECT name FROM sqlite_master WHERE type='table';", conn_old)
                for target_table in tables['name']:
                    df_db = pd.read_sql(f"SELECT * FROM {target_table}", conn_old)
                    process_classification(df_db)
                conn_old.close()
            finally:
                if os.path.exists(tmp_path): os.remove(tmp_path)

        # [Step 4] ë³‘í•© ë° ì •ì œ
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            
            # 1. SQL í˜¸í™˜ì„ ìœ„í•œ ì»¬ëŸ¼ëª… ì •ì œ (í•µì‹¬ ìˆ˜ì • ì‚¬í•­)
            # íŠ¹ìˆ˜ë¬¸ì ì œê±°, ê³µë°±ì€ ì–¸ë”ë°”ë¡œ ë³€ê²½
            clean_columns = []
            for col in combined_df.columns:
                clean_name = re.sub(r'\W+', '_', str(col)).strip('_') # íŠ¹ìˆ˜ë¬¸ì -> _
                if not clean_name or clean_name[0].isdigit(): # ìˆ«ìë¡œ ì‹œì‘í•˜ë©´ ì•ì— 'col_' ë¶™ì„
                    clean_name = 'col_' + clean_name
                clean_columns.append(clean_name)
            combined_df.columns = clean_columns

            # 2. íƒ€ì… ì •ì œ
            cols_to_fix = combined_df.select_dtypes(include=['object']).columns
            for col in cols_to_fix:
                combined_df[col] = combined_df[col].astype(str).replace(['nan', 'None'], '')

            combined_df = combined_df.drop_duplicates()

            # [Step 5] DB íŒŒì¼ ìƒì„± (OperationalError ë°©ì§€)
            db_filename = "integrated_sales_data.db"
            if os.path.exists(db_filename): os.remove(db_filename)
            
            try:
                conn_new = sqlite3.connect(db_filename)
                # chunksizeë¥¼ ì¶”ê°€í•˜ì—¬ ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì•ˆì •ì„± í™•ë³´
                combined_df.to_sql("total_data", conn_new, index=False, if_exists="replace", chunksize=1000)
                conn_new.close()
                status.update(label="í†µí•© ì™„ë£Œ!", state="complete", expanded=False)
            except Exception as e:
                st.error(f"âŒ DB ì €ì¥ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
                st.write("ì»¬ëŸ¼ ëª©ë¡ì„ í™•ì¸í•˜ì„¸ìš”:", combined_df.columns.tolist())

            st.subheader("ğŸ“Š í†µí•© ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
            st.dataframe(combined_df.head(10))
            with open(db_filename, "rb") as f:
                st.download_button("ğŸ’¾ í†µí•© DB ë‹¤ìš´ë¡œë“œ", f, file_name=db_filename)
else:
    st.info("íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
